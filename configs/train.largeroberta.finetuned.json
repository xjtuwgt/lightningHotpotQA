{
    "exp_name": "roberta_large_pytorch_finetuned_model",
    "model_type": "roberta",
    "encoder_name_or_path": "roberta-large",
    "num_train_epochs": 5,
    "per_gpu_train_batch_size": 2,
    "learning_rate": 1e-6,
    "optimizer": "RecAdam",
    "lr_scheduler": "cosine",
    "daug_type": "hgn_low",
    "gradient_accumulation_steps": 1,
    "gpus": 4,
    "fine_tuned_encoder": "roberta/roberta-large_hgn",
    "gnn_drop": 0.3,
    "bi_attn_drop": 0.3,
    "trans_drop": 0.2,
    "fp16": "False"
}