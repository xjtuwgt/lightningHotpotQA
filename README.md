<<<<<<< HEAD
The project is based on based on https://github.com/yuwfan/HGN
<!-- The project can be directly downloaded via https://github.com/xjtuwgt/lightningHotpotQA -->
<!-- which is based on https://github.com/yuwfan/HGN -->

<!-- 1. Requirements -->
<!--    a) Pytorch 1.6 + CUDA 10.2 -->
<!--    b) transformers: 3.3.1 -->
<!--    c) spacy: 2.3.2, and python -m spacy download en_core_web_lg -->
<!--    d) swifter: 1.0.7 -->
<!--    e) pytorch lightning: 1.0.8 -->
<!-- 2. Data/Preprocessed data download based on HGN (this is same as HGN: https://github.com/yuwfan/HGN) -->
<!--    a) bash scripts/download_data.sh -->
<!-- 3. For longformer-based retriever -->
<!--    a) copy model '/mnt/cephfs2/nlp/data/hotpotQA/finetuned/longformer_pytorchlighting_model.ckpt' to /data/models/finetuned/PS -->
<!-- 4. Download/get squad2/hgn based finetuned encoder -->
<!--    a) bash download_model.sh roberta --><!--  will download the data into the folder: data/models/pretrained -->
<!--       i) "ahotrod/roberta_large_squad2" -->
<!--       ii) "mfeb/albert-xxlarge-v2-squad2" -->
<!--    b) run 'python pretrainedExample' to convert all pretrained models (ahotrod, mfeb, hgn) to '.pkl' and saved at data/models/pretrained -->
<!-- 5. Get pre-processed data -->
<!--    a) run "bash jdrun.sh preprocess" --><!--  to generate hgn ranker and shuffle based hgn data -->
<!--    b) run "bash longformer_run.sh preprocess" --><!--  to generate longformer ranker and shuffle based data -->
<!-- 6. Train the models -->
<!--    a) pytorch DDP training -->
<!--       python -m torch.distributed.launch --nproc_per_node=4 jdtrain.py --config_file configs/train.largeroberta.lightning.finetuned.json -->
<!--    b) pytorch lightning based DDP training (if you familiar with pytorch-lightning) -->
<!--       python lightningtrain.py --config_file configs/train.largeroberta.lightning.json -->

<!--    Note for some hyper-parameters: -->
<!--    1) daug_type: data augmentation type: "hgn, hgn_low, hgn_reverse, hgn_low_reverse, long, long_low, -->
<!--    long_reverse, long_low_reverse" (the data generated by different rankers and pretrained encoders) -->
<!--    it is noted that i) for hotpotqa based encoder (roberta/roberta_large_hotpotqa): the tokenizer is case-sensitive; ii) for ahotrod based encoder: tokenizer is based ON LOWERCASED. -->
<!--    2) devf_type: "hgn, long" (validation over hgn ranker or long ranker) -->
<!--    3) fine_tuned_encoder: "ahotrod/roberta_large_squad2", "roberta/roberta_large_hotpotqa" --><!--  if this value is set as None, train from scratch -->


<!-- Need to run: -->
<!-- 1. Encoder == roberta/roberta_large_hotpotqa, data: long, long_reverse, hgn_reverse (hgn)/combine these data via dataugmentation/concat_data_processing.py; testing over hgn/long dev -->
<!-- 2. Encoder == ahotrod/roberta_large_squad2, data: long_low, long_reverse, hgn_low, hgn_low_reverse/combine these data via dataugmentation/concat_data_processing.py; testing over hgn/long dev -->


<!-- python -m torch.distributed.launch --nproc_per_node=4 jdattrain.py --config_file configs/train.largerobertatest.json -->
<!-- python -m torch.distributed.launch --nproc_per_node=4 jdtrain.py --config_file configs/train.largeroberta.example.json -->
<!-- python lightningtrain.py --config_file configs/train.largeroberta.lightning.json -->
<!-- python lightningtrain.py --config_file configs/train.largeroberta.lightning.finetuned.json -->
<!-- python -m torch.distributed.launch --nproc_per_node=4 train.py --config_file configs/train.albert.lightning.finetuned.json -->
<!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<!-- python jdpredict.py --config_file configs/predict.roberta.long.json -->
<!-- python jdpredict.py --config_file configs/predict.roberta.hgn.json -->
<!-- python jdpredict.py --config_file configs/predict.albert.long.json -->
<!-- python jdpredict.py --config_file configs/predict.albert.hgn.json -->
<!-- python lightningraphtrain.py --config_file configs/train.largeroberta.lightning.graph.finetuned.json -->
<!-- python lightningraphtrain.py --config_file configs/train.largeroberta.lightning.graph.finetuned2.json -->
<!-- python lightningraphtrain.py --config_file configs/train.largeroberta.lightning.graph.finetuned3.json -->

<!-- python lightningHGNPrediction.py --config_file configs/predict.roberta.lightning.long.json -->

<!-- python lightningtrain.py --config_file configs/train.largeroberta.lightning.finetuned.json -->
<!-- ======= -->

<!-- The project is based on https://github.com/yuwfan/HGN -->
<!-- >>>>>>> b5261a9873fb53ad877bc45ba63924d5c9c6c8f6 -->
